{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysuter/bme-labs/blob/main/session02/Ultrasound_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c4qSOQimrSM"
      },
      "source": [
        "# Classification from Ultradound images\n",
        "\n",
        "This example shows you a simple classification from ultrasouind data. It takes images from the BUSI dataset, where we have information for all images, if they contain a malignant or bening lesion, or if the tissue is healthy. We will try to directly infer this from the images.\n",
        "\n",
        "To make the data easier to handle, the data is resized to 200x200 pixels. \n",
        "\n",
        "## Enabling GPU Support\n",
        "\n",
        "To use GPU resources through Colab, change the runtime to GPU:\n",
        "\n",
        "    From the \"Runtime\" menu select \"Change Runtime Type\"\n",
        "    Choose \"GPU\" from the drop-down menu\n",
        "    Click \"SAVE\"\n",
        "\n",
        "This will reset the notebook and probably ask you if you are a robot (these instructions assume you are not). Running\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "in a cell will verify this has worked and show you what kind of hardware you have access to.\n",
        "\n",
        "\n",
        "First, we download the data and install some required packages. We will use MONAI and PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHv7DXlGqIYO"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/7o43bjdzfuul8l9/dataset-bmelab23.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset-bmelab23.zip\n",
        "dataroot = \"/content/dataset-bmelab23\""
      ],
      "metadata": {
        "id": "xZLciPY5FAqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/o4lpi26yufaaljd/busi_datainfo_fs23.csv"
      ],
      "metadata": {
        "id": "wKaW1sxEJOLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAuGIcpo4vOh"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \"monai[ignite, nibabel, torchvision, tqdm]==0.6.0\"\n",
        "!pip install -qU \"pandas\"\n",
        "!pip install -qU \"scikit-learn\"\n",
        "!pip install -qU \"captum\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check what kind of GPU we have available"
      ],
      "metadata": {
        "id": "VIU45WIAh_rG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pJkwNwYrT7x"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roeBS_RPrN7U"
      },
      "source": [
        "## Data handling\n",
        "Next, we prepare the data for training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHycObQkoqIx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from monai.data import ImageDataset\n",
        "\n",
        "subjectlist = sorted([e for e in os.listdir(dataroot) if 'mask' not in e])\n",
        "print(subjectlist[:10])\n",
        "\n",
        "imglist = np.array([os.path.join(dataroot, e) for e in subjectlist])\n",
        "print(imglist[:10])\n",
        "\n",
        "imglist_masks = np.array([e.split('.png')[0] + '_mask.png' for e in imglist])\n",
        "\n",
        "diagnosis = pd.read_csv(\"/content/busi_datainfo_fs23.csv\")\n",
        "\n",
        "# get encoded diagnosis for each subject\n",
        "diagnosis_list = np.array([diagnosis.loc[diagnosis[\"Filename\"] == e, :][\"Diagnosis_encoded\"].values[0] for e in subjectlist])\n",
        "diagnosis_raw_list = [diagnosis.loc[diagnosis[\"Filename\"] == e, :][\"Diagnosis\"].values[0] for e in subjectlist]\n",
        "\n",
        "# get overview of the class distribution\n",
        "print(diagnosis[\"Diagnosis\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vi9lqAo2qEq"
      },
      "source": [
        "## Data loading and augmentations\n",
        "The very high number of parameters in common neural networks can lead to overfitting. To make this harder, we perturb the training data. This is called \"data augmentation\". In this example, we randomly flip the image, rotate it, and add noise."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.transforms import Compose, AddChannel, NormalizeIntensity, \\\n",
        "  RandAdjustContrast, RandFlip, RandRotate, RandGaussianNoise\n",
        "\n",
        "# Data augmentation\n",
        "train_tfm = Compose([AddChannel(), NormalizeIntensity(),\n",
        "                     RandAdjustContrast(prob=0.1), \n",
        "                     RandFlip(prob=0.3), \n",
        "                     RandRotate(prob=0.1, range_x=0.2), # rotation +/- 11.5Â°\n",
        "                     RandGaussianNoise(prob=0.1)])\n",
        "\n",
        "test_tfm = Compose([AddChannel(), NormalizeIntensity()])"
      ],
      "metadata": {
        "id": "Q-OHcKLCNeON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLKkp9VvtI4P"
      },
      "source": [
        "Next, we create the dataset and random train/test split. Please note, that we would create train/validation/test splits for a real application, but omit this for the [sake of simplicity](https://xkcd.com/2587/). We reserve 25% of the data for testing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import transform\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from monai.data import ImageDataset, DataLoader\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "trainidx, testidx, y_train, y_test = train_test_split(np.arange(len(subjectlist)), \n",
        "                                                      diagnosis_list,\n",
        "                                                      test_size=0.25, \n",
        "                                                      shuffle=True,\n",
        "                                                      stratify=diagnosis_list,\n",
        "                                                      random_state=42)\n",
        "\n",
        "print(\"Size of the training and test datasets\")\n",
        "print(len(trainidx))\n",
        "print(len(testidx))\n",
        "\n",
        "batchsize = 200\n",
        "\n",
        "# create a validation data set and loader\n",
        "trainingset = ImageDataset(\n",
        "    image_files=imglist[trainidx],\n",
        "    labels=diagnosis_list[trainidx],\n",
        "    seg_files=imglist_masks[trainidx],\n",
        "    transform=train_tfm,\n",
        "    image_only=False,\n",
        "    transform_with_metadata=False,\n",
        ")\n",
        "train_loader = DataLoader(trainingset, batch_size=batchsize, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# create a validation data set and loader\n",
        "testset = ImageDataset(\n",
        "    image_files=imglist[testidx],\n",
        "    labels=diagnosis_list[testidx],\n",
        "    seg_files=imglist_masks[testidx],\n",
        "    transform=test_tfm,\n",
        "    image_only=False,\n",
        "    transform_with_metadata=False,\n",
        ")\n",
        "val_loader = DataLoader(testset, batch_size=batchsize, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "zDVPZwubRAou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make the whole thing reproducible by fixing the seeds"
      ],
      "metadata": {
        "id": "t0mqkHEsvr1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "torch.backends.cudnn.benchmark = False\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "4Y8g830BvrE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdM30hARpbDh"
      },
      "source": [
        "## The neural network\n",
        "\n",
        "Current state of the art networks probably have too many learnable parameters for this rather simple task. So we define a simpler network for this classification. It consists of "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48PTXS5cpw2o"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "      for m in self.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          nn.init.kaiming_normal_(m.weight)\n",
        "          if m.bias is not None:\n",
        "            nn.init.ones_(m.bias)\n",
        "          elif isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "            if m.bias is not None:\n",
        "              nn.init.ones_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.avg_pool2d(x, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, n_classes, n_input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=n_input_channels, out_channels=32, kernel_size=7, padding=3, stride=2),\n",
        "            nn.InstanceNorm2d(32),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        self.featextractor = nn.Sequential(\n",
        "            ConvBlock(in_channels=32, out_channels=64),\n",
        "            ConvBlock(in_channels=64, out_channels=128),\n",
        "            ConvBlock(in_channels=128, out_channels=256),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        self.drop_layer = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.fc = nn.Linear(256, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.featextractor(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop_layer(x)\n",
        "        out = self.fc(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_FttArf4CmC"
      },
      "source": [
        "Let's look at the classification model we created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUabv1Ll38fK"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "import torch \n",
        "\n",
        "# use a GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Classifier(n_classes=3, n_input_channels=1)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "summary(model, input_size=(1,200,200), batch_size=batchsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lxsVppSqRLr"
      },
      "source": [
        "## The loss and optimizer\n",
        "\n",
        "Next, we need a loss fuction and an optimizer. We will use the cross-entropy loss and the \"Adam\" optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzlGlbrX6EWy"
      },
      "outputs": [],
      "source": [
        "learning_rate = 4e-3\n",
        "\n",
        "classificationloss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYGCPB3v6PQJ"
      },
      "source": [
        "## Training\n",
        "We have everything set up, from the data to the network, so we can finally train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_0u_6Pb6dQR"
      },
      "outputs": [],
      "source": [
        "from monai.metrics import ROCAUCMetric, ConfusionMatrixMetric\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, \\\n",
        "    roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "epoch_num = 60\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = list()\n",
        "metric_values = list()\n",
        "auc_metric = ROCAUCMetric()\n",
        "accuracy = list()\n",
        "train_loss = list()\n",
        "val_loss = list()\n",
        "train_accuracy = list()\n",
        "val_accuracy = list()\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_loss_val = 0\n",
        "    step = 1\n",
        "\n",
        "    steps_per_epoch = len(trainingset) // train_loader.batch_size\n",
        "    pred_values = list()\n",
        "    label_values = list()\n",
        "    pred_values_test = list()\n",
        "    label_values_test = list()\n",
        "\n",
        "    # put the network in train mode; this tells the network and its modules to\n",
        "    # enable training elements such as normalisation and dropout, where applicable\n",
        "    model.train()\n",
        "    for train_batch in train_loader:\n",
        "\n",
        "      inputs, labels, info = train_batch[0].to(device), \\\n",
        "                              train_batch[2].to(device), \\\n",
        "                              train_batch[3]\n",
        "\n",
        "      # prepare the gradients for this step's back propagation\n",
        "      optimizer.zero_grad()\n",
        "        \n",
        "      # run the network forwards\n",
        "      outputs = model(inputs)\n",
        "        \n",
        "      # run the loss function on the outputs\n",
        "      loss = classificationloss(outputs, labels)\n",
        "        \n",
        "      # compute the gradients\n",
        "      loss.backward()\n",
        "        \n",
        "      # tell the optimizer to update the weights according to the gradients\n",
        "      # and its internal optimisation strategy\n",
        "      optimizer.step()\n",
        "\n",
        "      label_values += labels.cpu()\n",
        "      pred_values += outputs.argmax(dim=1).cpu()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      # print(f\"{step}/{len(trainingset) // trainloader.batch_size + 1}, training_loss: {loss.item():.4f}\")\n",
        "      step += 1\n",
        "\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    train_accuracy.append(accuracy_score(label_values, pred_values))\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "    print(\"Accuracy: \" + str(accuracy_score(label_values, pred_values)))\n",
        "    \n",
        "    # switch off training features of the network for this pass\n",
        "    model.eval()\n",
        "\n",
        "    # 'with torch.no_grad()' switches off gradient calculation for the scope of its context\n",
        "    with torch.no_grad():\n",
        "        # create lists to which we will concatenate the the validation results\n",
        "\n",
        "\n",
        "        val_step = 0\n",
        "        # iterate over each batch of images and run them through the network in evaluation mode\n",
        "        for val_batch in val_loader:\n",
        "          val_images, val_masks, val_labels, val_info = \\\n",
        "                                    val_batch[0].to(device), \\\n",
        "                                    val_batch[1].to(device), \\\n",
        "                                    val_batch[2].to(device), \\\n",
        "                                    val_batch[3]\n",
        "\n",
        "          # run the network\n",
        "          val_out = model(val_images)\n",
        "\n",
        "          test_loss = classificationloss(val_out, val_labels)\n",
        "          epoch_loss_val += test_loss.item()\n",
        "\n",
        "          predicted = val_out.argmax(dim=1).cpu()\n",
        "          pred_values_test += predicted\n",
        "          label_values_test += val_labels.cpu()\n",
        "          val_step += 1\n",
        "\n",
        "        val_accuracy.append(accuracy_score(label_values_test, pred_values_test))\n",
        "        epoch_loss_val /= val_step\n",
        "        val_loss.append(epoch_loss_val)           \n",
        "\n",
        "print(\"Done :-)\")\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can check how the loss and the accuracy evolves over the training process:"
      ],
      "metadata": {
        "id": "lw_U855XnPW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "epochs = np.arange(1, len(epoch_loss_values) +1)\n",
        "\n",
        "plt.figure(\"train/test\", (12, 6))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, epoch_loss_values)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.title(\"Test loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, val_loss)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.title(\"Training accuracy\")\n",
        "y = train_accuracy\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, train_accuracy)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.title(\"Test accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, val_accuracy)\n",
        "plt.suptitle(\"Ultrasound image classification\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pPRv0-ajA0EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate with a classification report and confusion matrix\n",
        "... like you already did in the first session"
      ],
      "metadata": {
        "id": "KulukEMIaNk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoding = {0: 'normal', 1: 'benign', 2: 'malignant'}"
      ],
      "metadata": {
        "id": "BPbFXe1kvn7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(label_values_test, pred_values_test, target_names=decoding.values()))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(label_values_test, pred_values_test, sample_weight=None,\n",
        "                                        display_labels=decoding.values(), \n",
        "                                        include_values=True, \n",
        "                                        xticks_rotation='horizontal', \n",
        "                                        values_format=None, cmap='viridis', \n",
        "                                        colorbar=True)\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.show()\n",
        "\n",
        "# And another normalized version\n",
        "ConfusionMatrixDisplay.from_predictions(label_values_test, pred_values_test, sample_weight=None, \n",
        "                                        normalize='all', \n",
        "                                        display_labels=decoding.values(), \n",
        "                                        include_values=True, \n",
        "                                        xticks_rotation='horizontal', \n",
        "                                        values_format=None, cmap='viridis', \n",
        "                                        ax=None, colorbar=True)\n",
        "plt.title(\"Normalized confusion matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ow448W05afMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model interpretation\n",
        "\n",
        "Now we can investigate what lead the model to the classification decision. We will use the library \"Captum\" for this."
      ],
      "metadata": {
        "id": "uygS83KIrjQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cf81hWPar0ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import LayerGradCam, LayerAttribution"
      ],
      "metadata": {
        "id": "sNU9JPGn2lca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can calculate the grad cam output. Here that is done for the second layer of the feature extractor. Check what the size of the attribution map is."
      ],
      "metadata": {
        "id": "L3Eocup9yNft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cam = LayerGradCam(model, layer=model.featextractor[1])\n",
        "attr = cam.attribute(val_images, val_labels)\n",
        "print(val_images[0].shape)\n",
        "print(attr[0].shape)"
      ],
      "metadata": {
        "id": "g1CHYzS86Wv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we upsample the images to match the input size (200x200 pixels). This can be done for the whole batch (in case this is our whole validation set) with one function call."
      ],
      "metadata": {
        "id": "XU7GhZTXyTHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upsampled_attr_val = LayerAttribution.interpolate(attr, (200, 200), \"area\")"
      ],
      "metadata": {
        "id": "LVl4oCLfU6Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overlay the grad cam result for the first sample in the validation set."
      ],
      "metadata": {
        "id": "UMJs8i6vylii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show axial slice\n",
        "print(val_labels[0])\n",
        "print(pred_values_test)\n",
        "print(val_batch[3]['filename_or_obj'][0])\n",
        "plt.imshow(torch.squeeze(val_images[0][0, :, :]).cpu().detach().numpy(), cmap='gray')\n",
        "plt.imshow(torch.squeeze(upsampled_attr_val[0]).cpu().detach().numpy(), alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZAwBup3s_oOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's map the encoded predictions and labes back to text"
      ],
      "metadata": {
        "id": "vwS194pO_Flq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can visualize the prediction and grad cam output for any sample you like in our validation set. Adapt valindex to select a different one."
      ],
      "metadata": {
        "id": "ZztI0jv-xnG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# select another sample with this index (0-194)\n",
        "valindex = 30\n",
        "\n",
        "samplename = os.path.split(val_info['filename_or_obj'][valindex])[-1].split('.png')[0]\n",
        "predicted_label = decoding[int(val_labels[valindex].cpu().numpy())]\n",
        "gt_label = decoding[int(val_out.argmax(dim=1)[valindex].cpu().numpy())]\n",
        "print(samplename)\n",
        "\n",
        "print(\"Predicted: \" + predicted_label)\n",
        "print(\"Ground truth: \" + gt_label)\n",
        "\n",
        "plt.figure(\"Model interpretation\", (16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Interpretation\")\n",
        "plt.imshow(torch.squeeze(val_images[valindex][0, :, :]).cpu().detach().numpy(), cmap='gray')\n",
        "plt.imshow(torch.squeeze(upsampled_attr_val[valindex]).cpu().detach().numpy(), alpha=0.5)\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Label map\")\n",
        "plt.imshow(torch.squeeze(val_masks[valindex]).cpu().detach().numpy(), cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.suptitle(\"Sample \" + samplename + \" predicted: \" + predicted_label + \", true: \" + gt_label)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u8y1Alyf_JuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO9EPf3XphQRQHqckprRe/G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}