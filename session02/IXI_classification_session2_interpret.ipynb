{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysuter/bme-labs/blob/main/session02/IXI_classification_session2_interpret.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c4qSOQimrSM"
      },
      "source": [
        "# Classification from MRI images\n",
        "\n",
        "This example gives a brief example on how to do a simple classification from MRI data. It takes images from the IXI dataset, where we have information about the sex of the subject alongside an MRI. We will try to directly infer the sex of a given patient from MRI. It is heavily inspired by this MONAI example: https://colab.research.google.com/github/Project-MONAI/MONAIBootcamp2021/blob/master/day1/3.%20End-To-End%20Workflow%20with%20MONAI.ipynb \n",
        "MONAI is a framework built on PyTorch specifically for deep learning with medical imaging data.\n",
        "\n",
        "To make the data easier to handle, the data is resized to a volume of 32x32x32 voxels. Preprocessing further included cropping to the foreground and z-score normalization.\n",
        "\n",
        "## Enabling GPU Support\n",
        "\n",
        "To use GPU resources through Colab, change the runtime to GPU:\n",
        "\n",
        "    From the \"Runtime\" menu select \"Change Runtime Type\"\n",
        "    Choose \"GPU\" from the drop-down menu\n",
        "    Click \"SAVE\"\n",
        "\n",
        "This will reset the notebook and probably ask you if you are a robot (these instructions assume you are not). Running\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "in a cell will verify this has worked and show you what kind of hardware you have access to.\n",
        "\n",
        "\n",
        "First, we download the data and install some required packages. We will use MONAI and PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHv7DXlGqIYO"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/si4zrl1vuv1p369/ixi_t1_crop_80.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAuGIcpo4vOh"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \"monai[ignite, nibabel, torchvision, tqdm]==0.6.0\"\n",
        "!pip install -qU \"pymia\"\n",
        "!pip install -qU \"torchio\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pJkwNwYrT7x"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roeBS_RPrN7U"
      },
      "source": [
        "## Data Loading\n",
        "The data we use comes in the hdf5 format. While we could also just load the individual images, this is more efficient. Data handling often is a bottleneck in the pipeline.\n",
        "\n",
        "To handle this dataset, we need some auxiliaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHycObQkoqIx"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class H5Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, h5_path, transform=None):\n",
        "        super(H5Dataset, self).__init__()\n",
        "        h5_file = h5py.File(h5_path, 'r')\n",
        "        self.images = h5_file['data']['images']\n",
        "        self.labels = h5_file['data']['gt']\n",
        "        self.name = h5_file['meta']['subjects']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index_raw = int(index)\n",
        "        index = str(index).zfill(3)\n",
        "        img = torch.from_numpy(self.images[index][:]).float()\n",
        "        label = torch.from_numpy(self.labels[index][:]).to(torch.int64)\n",
        "        subjects = self.name[index_raw][:].decode('utf-8')\n",
        "        return {\"images\": img,\n",
        "                \"labels\": label,\n",
        "                \"subjects\": subjects}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdM30hARpbDh"
      },
      "source": [
        "## The neural network\n",
        "\n",
        "Current state of the art networks probably have too many learnable parameters for this rather simple task. So we define a simpler network for this classification. It consists of "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48PTXS5cpw2o"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.InstanceNorm3d(out_channels),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv3d(out_channels, out_channels, 3, 1, 1),\n",
        "            nn.InstanceNorm3d(out_channels),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.ones_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.ones_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.avg_pool3d(x, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, n_classes, n_input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv3d(in_channels=n_input_channels, out_channels=32, kernel_size=7, padding=3, stride=2),\n",
        "            nn.InstanceNorm3d(32),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        self.featextractor = nn.Sequential(\n",
        "            ConvBlock(in_channels=32, out_channels=64),\n",
        "            ConvBlock(in_channels=64, out_channels=128),\n",
        "            ConvBlock(in_channels=128, out_channels=256),\n",
        "            nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        )\n",
        "\n",
        "        self.drop_layer = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.fc = nn.Linear(256, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.featextractor(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop_layer(x)\n",
        "        out = self.fc(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLKkp9VvtI4P"
      },
      "source": [
        "Next, we load the dataset and randomly create a train/test split. Please note, that we would create train/validation/test splits for a real application, but omit this for the [sake of simplicity](https://xkcd.com/2587/). We reserve 25% of the data for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QhFtv8mrpGW"
      },
      "outputs": [],
      "source": [
        "dataset = H5Dataset(\"/content/ixi_t1_crop_80.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEM9J_3kCv33"
      },
      "outputs": [],
      "source": [
        "print(len(dataset.images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bm0XteXr36d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "subjectlist = np.arange(len(dataset.images))\n",
        "\n",
        "print(list(dataset.images.keys()))\n",
        "\n",
        "# Reserve 25% of the data for testing\n",
        "subj_train, subj_test = train_test_split(subjectlist, test_size=0.25, \n",
        "                                         shuffle=True, random_state=42)\n",
        "\n",
        "trainingset = torch.utils.data.Subset(dataset, subj_train)\n",
        "testset = torch.utils.data.Subset(dataset, subj_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vi9lqAo2qEq"
      },
      "source": [
        "## Data loading and augmentations\n",
        "The very high number of parameters in common neural networks can lead to overfitting. To make this harder, we perturb the training data. This is called \"data augmentation\". In this example, we randomly flip the image, transform it with an affine matrix, add a bias field, noise and blurring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83rFB8aZ242R"
      },
      "outputs": [],
      "source": [
        "import torchio as tio\n",
        "\n",
        "import pymia.data.transformation as tfm\n",
        "import pymia.data.definition as defs\n",
        "\n",
        "\n",
        "class TorchIOTransform(tfm.Transform):\n",
        "    \"\"\"Example wrapper for `TorchIO <https://github.com/fepegar/torchio>`_ transformations.\"\"\"\n",
        "\n",
        "    def __init__(self, transforms: list, entries=(defs.KEY_IMAGES, defs.KEY_LABELS)) -> None:\n",
        "        super().__init__()\n",
        "        self.transforms = transforms\n",
        "        self.entries = entries\n",
        "\n",
        "    def __call__(self, sample: dict) -> dict:\n",
        "        # unsqueeze samples to be 4-D tensors, as required by TorchIO\n",
        "        for entry in self.entries:\n",
        "            if entry not in sample:\n",
        "                if tfm.raise_error_if_entry_not_extracted:\n",
        "                    raise ValueError(tfm.ENTRY_NOT_EXTRACTED_ERR_MSG.format(entry))\n",
        "                continue\n",
        "\n",
        "            np_entry = tfm.check_and_return(sample[entry], np.ndarray)\n",
        "            sample[entry] = np.expand_dims(np_entry, -1)\n",
        "\n",
        "        # apply TorchIO transforms\n",
        "        for t in self.transforms:\n",
        "            sample = t(sample)\n",
        "\n",
        "        # squeeze samples back to original format\n",
        "        for entry in self.entries:\n",
        "            np_entry = tfm.check_and_return(sample[entry].numpy(), np.ndarray)\n",
        "            sample[entry] = np_entry.squeeze(-1)\n",
        "\n",
        "        return sample\n",
        "\n",
        "transforms_augmentation = [TorchIOTransform(\n",
        "    [tio.RandomFlip(axes='LR', flip_probability=0.5), \n",
        "     tio.RandomFlip(axes='AP', flip_probability=0.5),\n",
        "     tio.RandomAffine(scales=(0.85, 1.15), degrees=10, \n",
        "                      isotropic=False, default_pad_value='otsu',\n",
        "                      image_interpolation='NEAREST'),\n",
        "     tio.RandomBiasField(),\n",
        "     tio.RandomNoise(),\n",
        "     tio.RandomBlur(),\n",
        "     ])]\n",
        "\n",
        "batchsize = 4\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainingset, batch_size=batchsize, \n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          pin_memory=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize, \n",
        "                                         shuffle=False, num_workers=2, \n",
        "                                         pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_FttArf4CmC"
      },
      "source": [
        "Let's look at the classification model we created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUabv1Ll38fK"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "assert torch.cuda.is_available(), \"GPU not available\"\n",
        "device = torch.device(\"cuda:0\")\n",
        "model = Classifier(n_classes=2, n_input_channels=1)\n",
        "model.to(device)\n",
        "print(model)\n",
        "summary(model, (1, 80, 80, 80))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lxsVppSqRLr"
      },
      "source": [
        "## The loss and optimizer\n",
        "\n",
        "Next, we need a loss fuction and an optimizer. We will use the cross-entropy loss and the \"Adam\" optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzlGlbrX6EWy"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4\n",
        "\n",
        "classificationloss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYGCPB3v6PQJ"
      },
      "source": [
        "## Training\n",
        "We have everything set up, from the data to the network, so we can finally train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_0u_6Pb6dQR"
      },
      "outputs": [],
      "source": [
        "from monai.metrics import ROCAUCMetric, ConfusionMatrixMetric\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, \\\n",
        "    roc_auc_score\n",
        "\n",
        "epoch_num = 20\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = list()\n",
        "metric_values = list()\n",
        "auc_metric = ROCAUCMetric()\n",
        "accuracy = list()\n",
        "train_loss = list()\n",
        "val_loss = list()\n",
        "train_accuracy = list()\n",
        "val_accuracy = list()\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_loss_val = 0\n",
        "    step = 1\n",
        "\n",
        "    steps_per_epoch = len(trainingset) // trainloader.batch_size\n",
        "    pred_values = list()\n",
        "    label_values = list()\n",
        "    pred_values_test = list()\n",
        "    label_values_test = list()\n",
        "\n",
        "    # put the network in train mode; this tells the network and its modules to\n",
        "    # enable training elements such as normalisation and dropout, where applicable\n",
        "    model.train()\n",
        "    for batch in trainloader:\n",
        "\n",
        "      inputs, labels = batch[\"images\"].swapaxes(1, -1).cuda(), batch[\"labels\"].squeeze(1).cuda()\n",
        "\n",
        "      # prepare the gradients for this step's back propagation\n",
        "      optimizer.zero_grad()\n",
        "        \n",
        "      # run the network forwards\n",
        "      outputs = model(inputs)\n",
        "        \n",
        "      # run the loss function on the outputs\n",
        "      loss = classificationloss(outputs, labels)\n",
        "        \n",
        "      # compute the gradients\n",
        "      loss.backward()\n",
        "        \n",
        "      # tell the optimizer to update the weights according to the gradients\n",
        "      # and its internal optimisation strategy\n",
        "      optimizer.step()\n",
        "\n",
        "      label_values += labels.cpu()\n",
        "      pred_values += outputs.argmax(dim=1).cpu()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      # print(f\"{step}/{len(trainingset) // trainloader.batch_size + 1}, training_loss: {loss.item():.4f}\")\n",
        "      step += 1\n",
        "\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    train_accuracy.append(accuracy_score(label_values, pred_values))\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "    print(\"Accuracy: \" + str(accuracy_score(label_values, pred_values)))\n",
        "    \n",
        "    # switch off training features of the network for this pass\n",
        "    model.eval()\n",
        "\n",
        "    # 'with torch.no_grad()' switches off gradient calculation for the scope of its context\n",
        "    with torch.no_grad():\n",
        "        # create lists to which we will concatenate the the validation results\n",
        "\n",
        "\n",
        "        val_step = 0\n",
        "        # iterate over each batch of images and run them through the network in evaluation mode\n",
        "        for val_data in testloader:\n",
        "            val_images, val_labels = val_data[\"images\"].swapaxes(1, -1).cuda(), val_data[\"labels\"].squeeze(1).cuda()\n",
        "\n",
        "            # run the network\n",
        "            val_out = model(val_images)\n",
        "\n",
        "            test_loss = classificationloss(val_out, val_labels)\n",
        "            epoch_loss_val += test_loss.item()\n",
        "\n",
        "            pred_values_test += val_out.argmax(dim=1).cpu()\n",
        "            label_values_test += val_labels.cpu()\n",
        "            val_step += 1\n",
        "\n",
        "        val_accuracy.append(accuracy_score(label_values_test, pred_values_test))\n",
        "        epoch_loss_val /= val_step\n",
        "        val_loss.append(epoch_loss_val)           \n",
        "\n",
        "print(\"Done :-)\")\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can check how the loss and the accuracy evolves over the training process:"
      ],
      "metadata": {
        "id": "lw_U855XnPW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = np.arange(1, len(epoch_loss_values) +1)\n",
        "\n",
        "plt.figure(\"train/test\", (12, 6))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, epoch_loss_values)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.title(\"Test loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, val_loss)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.title(\"Training accuracy\")\n",
        "y = train_accuracy\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, train_accuracy)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.title(\"Test accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(epochs, val_accuracy)\n",
        "plt.suptitle(\"Classification on the IXI dataset (female/male)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pPRv0-ajA0EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model interpretation\n",
        "\n",
        "Now we can investigate what lead the model to the classification decision. We will use the library \"Captum\" for this."
      ],
      "metadata": {
        "id": "uygS83KIrjQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cf81hWPar0ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"captum\""
      ],
      "metadata": {
        "id": "QXzrd5VY2Vu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import LayerGradCam, LayerAttribution"
      ],
      "metadata": {
        "id": "sNU9JPGn2lca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cam = LayerGradCam(model, layer=model.featextractor[3])"
      ],
      "metadata": {
        "id": "YRDRacb05-rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cam = LayerGradCam(model, layer=model.featextractor[1])\n",
        "attr = cam.attribute(val_images, val_labels)\n",
        "print(inputs.shape)\n",
        "print(attr.shape)"
      ],
      "metadata": {
        "id": "g1CHYzS86Wv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape\n",
        "upsampled_attr_val = LayerAttribution.interpolate(attr, torch.squeeze(inputs[0]).shape, \"trilinear\")\n",
        "upsampled_attr_val.shape"
      ],
      "metadata": {
        "id": "LVl4oCLfU6Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map the class values to male/female\n",
        "decoding = {0: 'male', 1: 'female'}\n",
        "print(inputs.shape)"
      ],
      "metadata": {
        "id": "BfqiqB5xHscU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select another sample with this index (0-3)\n",
        "valindex = 1\n",
        "\n",
        "gt_label = decoding[int(val_labels[valindex].cpu().numpy())]\n",
        "predicted_label = decoding[int(val_out.argmax(dim=1)[valindex].cpu().numpy())]\n",
        "\n",
        "print(\"Predicted: \" + predicted_label)\n",
        "print(\"Ground truth: \" + gt_label)\n",
        "# show axial slice\n",
        "plt.imshow(np.flipud(torch.squeeze(inputs[valindex][0, :, :, :]).cpu().detach().numpy()[:, :, 40]), cmap='gray')\n",
        "plt.imshow(np.flipud(torch.squeeze(upsampled_attr_val[valindex]).cpu().detach().numpy()[:, :, 40]), alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# show sagittal slice\n",
        "plt.imshow(np.rot90(torch.squeeze(inputs[valindex][0, :, :, :]).cpu().detach().numpy()[:, 40, :], (1)), cmap='gray')\n",
        "plt.imshow(np.rot90(torch.squeeze(upsampled_attr_val[valindex]).cpu().detach().numpy()[:, 40, :], (1)), alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# show coronal slice\n",
        "plt.imshow(np.rot90(torch.squeeze(inputs[valindex][0, :, :, :]).cpu().detach().numpy()[40, :, :], (1)), cmap='gray')\n",
        "plt.imshow(np.rot90(torch.squeeze(upsampled_attr_val[valindex]).cpu().detach().numpy()[40, :, :], (1)), alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZAwBup3s_oOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get maximum attribution and plot\n",
        "maxidx = np.unravel_index(upsampled_attr_val[valindex].cpu().argmax(), upsampled_attr_val[0].cpu().shape)\n",
        "print(maxidx)\n",
        "\n",
        "plt.imshow(np.flipud(torch.squeeze(inputs[valindex][0, :, :, :]).cpu().detach().numpy()[:, :, maxidx[3]]), cmap='gray')\n",
        "plt.imshow(np.flipud(torch.squeeze(upsampled_attr_val[valindex]).cpu().detach().numpy()[:, :, maxidx[3]]), alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(np.rot90(torch.squeeze(inputs[valindex][0, :, :, :]).cpu().detach().numpy()[:, maxidx[2], :], (1)), cmap='gray')\n",
        "plt.imshow(np.rot90(torch.squeeze(upsampled_attr_val[valindex]).cpu().detach().numpy()[:, maxidx[2], :], (1)), alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(np.rot90(torch.squeeze(inputs[valindex][0, :, :, :]).cpu().detach().numpy()[maxidx[1], :, :], (1)), cmap='gray')\n",
        "plt.imshow(np.rot90(torch.squeeze(upsampled_attr_val[valindex]).cpu().detach().numpy()[maxidx[1], :, :], (1)), alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "YSNQfY_2bYVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vwS194pO_Flq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u8y1Alyf_JuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}